# PACKAGES
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE
from numpy.random import seed
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
rom sklearn.tree import DecisionTreeClassifier
from imblearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.neighbors import KNeighborsRegressor
from sklearn import metrics
import itertools
import seaborn as sns
from sklearn.metrics import roc_curve, auc
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

# IMPORT DATA
df=pd.read_csv('bs140513_032310.csv')
df=df.drop(['zipMerchant', 'zipcodeOri'], axis=1)

# DATA PRE-PROCESSING
# Converting categorical variables into numerical
col_categorical = df.select_dtypes(include= ['object']).columns
for col in col_categorical:
    df[col] = df[col].astype('category')
df[col_categorical] = df[col_categorical].apply(lambda x: x.cat.codes)

# INDEPENDENT AND RESPONSE VARIABLE
X=df.drop('fraud', axis=1) # Independent (We drop the fraud column)
y=df['fraud'] # Response (fraud column)

# FUNCTION FOR PLOTTING ROC-AUC CURVE
def plot_roc_auc(y_test, preds):
    fpr, tpr, threshold = roc_curve(y_test, preds)
    roc_auc = auc(fpr, tpr)
    plt.title('Receiver Operating Characteristic', fontsize = 18)
    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
    plt.legend(loc = 'lower right')
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate', fontsize = 20)
    plt.xlabel('False Positive Rate', fontsize = 20)
    plt.show()
    
# FUNCTION TO PLOT THE CONFUSION MATRIX
class_names=np.array(['0','1']) # Class = 1 (fraud), Class = 0 (non fraud)

def plot_confusion_matrix(cm, classes,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title, fontsize = 22)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = 'd' 
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label', fontsize = 22)
    plt.xlabel('Predicted label', fontsize = 22)
    
# SET SEED
seed(71735164)

# SPLIT THE DATA INTO TRAIN AND TEST
X_train, X_test, y_train, y_test = train_test_split(X_us,y_us,test_size=0.2)


# K-NEAREST NEIGHBORS
# define model
model = KNeighborsClassifier(n_neighbors=5,p=1)

# define evaluation procedure
cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)

# define pipeline
over = SMOTE(sampling_strategy=0.05) # minority class=5% of the mayority class
under = RandomUnderSampler(sampling_strategy=0.5) # majority class down to 50 percent
steps = [('o', over), ('u', under), ('m', model)]
clf = Pipeline(steps=steps)

clf = clf.fit(X_train, y_train)

# evaluate model
scores = cross_val_score(clf, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)

y_pred_knn = clf.predict(X_test)

print("\n Classification Report for K-Nearest Neighbours: \n",
      classification_report(y_test, y_pred_knn))
plot_roc_auc(y_test, clf.predict_proba(X_test)[:,1])

cm = confusion_matrix(y_test, y_pred_knn)
plot_confusion_matrix(cm,class_names)


# RANDOM FOREST CLASSIFIER
# define the model
model_RF =  RandomForestClassifier(n_estimators=100,max_depth=8,random_state=42,
                                verbose=1,class_weight="balanced", n_jobs= -1 )

# define evaluation procedure
cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)

# define pipeline
over_RF = SMOTE(sampling_strategy=0.06) # minority class=6% of the mayority class
under_RF = RandomUnderSampler(sampling_strategy=0.5) # majority class down to 50 percent
steps_RF = [('o', over_RF), ('u', under_RF), ('m', model_RF)]
clf_RF = Pipeline(steps=steps_RF)

clf_RF = clf_RF.fit(X_train, y_train)

# evaluate model
scores_RF = cross_val_score(clf_RF, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)

y_pred_RF = clf_RF.predict(X_test)

print("\n Classification Report for K-Nearest Neighbours: \n",
      classification_report(y_test, y_pred_RF))
plot_roc_auc(y_test, clf_RF.predict_proba(X_test)[:,1])

cm = confusion_matrix(y_test, y_pred_RF)
plot_confusion_matrix(cm,class_names)


# XGBOOST CLASSIFIER
# define model
model_XGB =   xgb.XGBClassifier(max_depth=6, learning_rate=0.003, n_estimators=486, 
                                objective="binary:hinge", booster='gbtree', 
                                n_jobs=-1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, 
                                subsample=0.9, colsample_bytree=0.9, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, 
                                 base_score=0.5, random_state=42)


# define evaluation procedure
cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)

#define pipeline
over_XGB = SMOTE(sampling_strategy=0.1) # minority class=10% of the mayority class
under_XGB = RandomUnderSampler(sampling_strategy=0.5) # majority class down to 50 percent
steps_XGB = [('o', over_XGB), ('u', under_XGB), ('m', model_XGB)]
clf_XGB = Pipeline(steps=steps_XGB)

clf_XGB = clf_XGB.fit(X_train, y_train)

# evaluate model
scores_XGB = cross_val_score(clf_XGB, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)

y_pred_XGB = clf_XGB.predict(X_test)

print("\n Classification Report for K-Nearest Neighbours: \n",
      classification_report(y_test, y_pred_XGB))
plot_roc_auc(y_test, clf_XGB.predict_proba(X_test)[:,1])

cm = confusion_matrix(y_test, y_pred_XGB)
plot_confusion_matrix(cm,class_names)
